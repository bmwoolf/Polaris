v4

────────────────────────────────────────────────────────────────────────
M1: BetaReg-MLP (Deep Net)
────────────────────────────────────────────────────────────────────────
INPUTS
  [ x_cell ] 
  [ x_capsid ] 
  [ x_promoter ]
        │
        ▼
    [ CONCAT ]
        │
    [ LayerNorm ]
        │
        ▼
SUBCOMPONENTS
  [ Dense 896→512 ] → [ SiLU ] → [ Dropout ]
        │
        ▼
  [ Dense 512→256 ] → [ SiLU ] → [ Dropout ]
        │
        ▼
  [ Dense 256→128 ] → [ SiLU ]
        │
        ▼
OUTPUT HEAD
  [ Linear 128→2 ] → [ Softplus ] → (α̂, β̂)
        │
        ▼
OUTPUT
  Beta(α̂,β̂) → Expected % expression p̂
               + Confidence Interval (uncertainty)



────────────────────────────────────────────────────────────────────────
M2: Cross-Attention Network
────────────────────────────────────────────────────────────────────────

x_cell (cell embedding) ────────────────┐
                                        │
                                        ▼
                             [ Linear Projection → Query (Q) ]

x_capsid (capsid embedding) ─┐
                             ├──► [ Linear Projection → Keys (K) ]
x_promoter (promoter vector) ─┘
                             └──► [ Linear Projection → Values (V) ]

Query (Q) ───────────────────────────────────────┐
Keys (K), Values (V) ────────────────────────────┘
                                        │
                                        ▼
                               [ Multi-Head Attention ]
                                        │
                                        ▼
                               [ Residual + LayerNorm ]
                                        │
                                        ▼
                               [ Feedforward Layer ]
                                        │
                                        ▼
                               [ Residual + LayerNorm ]
                                        │
                                        ▼
Concatenate (attn_out + x_cell + x_capsid + x_promoter)
                                        │
                                        ▼
                                   [ LayerNorm ]
                                        │
                                        ▼
                        [ Dense →128 ] → [ SiLU ] → [ Linear 128→2 ]
                                        │
                                        ▼
                              [ Softplus → (α̂, β̂) ]
                                        │
                                        ▼
OUTPUT: Beta(α̂,β̂)
        • Expected % expression (p̂ = α̂/(α̂+β̂))
        • Confidence Interval (uncertainty)


────────────────────────────────────────────────────────────────────────
M3: Gradient-Boosted Trees
────────────────────────────────────────────────────────────────────────

LEARNED EMBEDDINGS
 ┌───────────────────────────────┐
 │ x_cell (from scVI→MLP)        │
 │ x_capsid (Evo2∥ESM2→fuse)     │
 │ x_promoter (meta MLP ± CNN)   │
 └───────────────────────────────┘
            │
            ├──────────────┐
            │              │
            ▼              ▼
HANDCRAFTED FEATURE DERIVATION             METADATA DIRECT
 ┌───────────────────────────────┐         ┌───────────────────────────────┐
 │ cosine(cell,capsid)           │         │ tissue flags (one-hot)        │
 │  = dot/‖·‖‖·‖                 │         │ promoter strength (scaled)    │
 └───────────────────────────────┘         │ MOI → log1p                   │
            │                              │ timepoint → log1p / bins      │
            ▼                              └───────────────────────────────┘
                 ┌────────────────────────────────────────────────────────┐
                 │                 FEATURE MATRIX  Z                      │
                 │ concat[ x_cell, x_capsid, x_promoter,                  │
                 │         cosine_sim, vec_tissue, strength,              │
                 │         moi_log, t_feat ]                              │
                 └────────────────────────────────────────────────────────┘
                                             │
                                             ▼
                                 GRADIENT-BOOSTED TREES (XGBoost)
                                 ┌────────────────────────────────┐
                                 │ Tree #1  Tree #2  …  Tree #T   │
                                 └────────────────────────────────┘
                                             │
                                             ▼
                                  ENSEMBLE SUM → raw_score = logit(y)
                                             │
                                             ▼
                            OPTIONAL CALIBRATION (Platt / Isotonic)
                                             │
                                             ▼
                                            OUTPUT
                            ┌───────────────────────────────┐
                            │ p̂ = sigmoid(raw_score)        │
                            │ (calibrated probability)      │
                            └───────────────────────────────┘



────────────────────────────────────────────────────────────────────────
M4: Sparse Gaussian Process (RBF)
────────────────────────────────────────────────────────────────────────

INPUTS
 ┌───────────────────────────────┐
 │ x_cell, x_capsid, x_promoter  │
 └───────────────────────────────┘
              │
              ▼
 [ CONCAT ] → high-dim vector (896D)
              │
              ▼
 ┌───────────────────────────────┐
 │ PCA REDUCTION                 │
 │ • 896D → 32D (x₃₂)            │
 └───────────────────────────────┘
              │
              ▼
 ┌────────────────────────────────────────────┐
 │   SPARSE GAUSSIAN PROCESS (FITC approx)    │
 │                                            │
 │  [1] Inputs: x₃₂, inducing points Z        │
 │  [2] Kernels: RBF + White noise            │
 │  [3] Covariance: K_mm, K_nm, K_nn approx   │
 │  [4] Variational / FITC inference          │
 │  [5] Prediction: μ(x), σ²(x)               │
 │  [6] Link: sigmoid(μ) → p̂                  │
 └────────────────────────────────────────────┘
              │
              ▼
 OUTPUTS
   • p̂ = predicted probability
   • σ²(x) = uncertainty



────────────────────────────────────────────────────────────────────────
M5: Linear Probe (Ridge)
────────────────────────────────────────────────────────────────────────

INPUTS (learned embeddings)
┌──────────────────────────────────────────┐
│ x_cell  │ x_capsid  │ x_promoter         │
└──────────────────────────────────────────┘
                │
                ▼
[ CONCAT ] → x = [x_cell ⊕ x_capsid ⊕ x_promoter]
                │
                ▼
┌──────────────────────────────────────────┐
│ STANDARDIZE x                            │
│  • zero-mean, unit-variance per feature  │
└──────────────────────────────────────────┘
                │
                ▼
┌──────────────────────────────────────────┐
│ LINEAR MAP                               │
│  • s = w·x + b                           │
└──────────────────────────────────────────┘
                │
                ▼
┌──────────────────────────────────────────┐
│ SIGMOID                                  │
│  • p̂ = σ(s)                              │
└──────────────────────────────────────────┘
                │
                ├───────────────► INFERENCE OUTPUT
                │                 ┌──────────────────────────────┐
                │                 │ p̂ (baseline probability)     │
                │                 └──────────────────────────────┘
                │
                ▼
TRAINING (uses computational label y)
┌──────────────────────────────────────────────────────────────────────┐
│ LABEL FACTORY (computational y)                                      │
│  inputs: x_cell, x_capsid, x_promoter, strength, MOI, time, tissue   │
│  priors:                                                             │
│    • P_cc = σ(α·cos(x_cell, x_capsid))                               │
│    • P_pr = normalize(strength; tissue context)                      │
│    • A   = g(MOI, time)                                              │
│  fusion (monotone):                                                  │
│    • s* = w1·logit(P_cc) + w2·logit(P_pr) + w3·A + b*                │
│    • y  = σ(s*) ∈ [0,1]                                              │
└──────────────────────────────────────────────────────────────────────┘
                │
                ▼
┌──────────────────────────────────────────────────────────────────────┐
│ LOSS                                                                 │
│  compare p̂ vs y (from label factory)                                 │
│   • Logistic:  𝓛_cls = −[y log p̂ + (1−y) log(1−p̂)]                   │
│   • (or) Squared on score:  𝓛_reg = (y − s)²                         │
│  ridge penalty:  𝓛_ridge = λ‖w‖²                                     │
│  total:  𝓛 = 𝓛_cls/𝓛_reg + 𝓛_ridge                                   │
└──────────────────────────────────────────────────────────────────────┘
                │
                ▼
┌──────────────────────────────────────────────────────────────────────┐
│ SOLVER (minimizes 𝓛 over w,b)                                        │
│  • closed-form ridge:  w = (XᵀX + λI)⁻¹ Xᵀy  (small/medium X)        │
│  • coordinate descent / CG / SGD  (high-dim)                         │
│  updates w,b in [LINEAR MAP]                                         │
└──────────────────────────────────────────────────────────────────────┘



─────────────────────────────────────────────────────────────────────────────
STACKING BLENDER (Ensemble Combiner)
────────────────────────────────────────────────────────────────────────────
INPUTS (from M1-M5 base models)
┌──────────────────────────────────────────────────────────────────────────┐
│ p₁, p₂, p₃, p₄, p₅  (probabilities)                                      │
│ Optional uncertainty:                                                    │
│  • M1/M2: Beta(α̂,β̂)   • M4: Logit-N(μ,σ²)   • M3/M5: point probs         │
└──────────────────────────────────────────────────────────────────────────┘
                                  │
                                  ▼
A) META-LEARNING (STACKING, OOF ONLY)
┌──────────────────────────────────────────────────────────────────────────┐
│ Build features (logit space):                                            │
│   zᵢ^(m) = logit(clip(pᵢ^(m), ε, 1−ε))                                   │
│ Constrained ridge on logits (softmax weights):                           │
│   w = softmax(u)  ⇒  w_m ≥ 0,  Σ w_m = 1                                 │
│   sᵢ = b + Σ_m w_m zᵢ^(m)                                                │
│   Minimize:  Σᵢ ( yᵢ − σ(sᵢ) )² + λ‖w‖²                                   │
│ Store:  w*, b*                                                           │
└──────────────────────────────────────────────────────────────────────────┘
                                  │
                                  ▼
B) UNCERTAINTY PROPAGATION (INFERENCE)
┌──────────────────────────────────────────────────────────────────────────┐
│ Monte-Carlo blending (t = 1..T):                                         │
│   sample p_t^(m):                                                        │
│     • Beta:  p_t^(m) ~ Beta(α̂_m,β̂_m)                                     │
│     • GP:    s_t^(4) ~ N(μ₄,σ₄²),  p_t^(4)=σ(s_t^(4))                    │
│     • Point: p_t^(m)=p^(m) (or tiny logit noise)                         │
│   z_t^(m) = logit(clip(p_t^(m), ε, 1−ε))                                 │
│   s_t = b* + Σ_m w*_m z_t^(m)                                            │
│   q_t = σ(s_t)                                                           │
│ Aggregate:  p̄ = (1/T)Σ q_t,  Var[p] = (1/(T−1))Σ (q_t − p̄)²              │
│ (Alt analytic on logits: μ_s = b*+Σ w*_m μ_m, v_s = Σ w*_m² v_m;         │
│  delta method → Var[p] ≈ (σ(μ_s)(1−σ(μ_s)))² v_s )                       │
└──────────────────────────────────────────────────────────────────────────┘
                                  │
                                  ▼
C) CALIBRATION & INTERVALS
┌──────────────────────────────────────────────────────────────────────────┐
│ Isotonic calibration (held-out):                                         │
│   p̂ = f_iso(p̄)    and    p̂_t = f_iso(q_t)                                │
│ Model quantile band:  [q05, q95] from {p̂_t}                              │
│ Split-conformal (per stratum g):                                         │
│   r_j = | y_j − f_iso(p_blend,j) |                                       │
│   q_α = Quantile_{1−α}({r_j}_{j∈g})                                      │
│   Final CI:  [L,U] = [ max(0, p̂ − q_α),  min(1, p̂ + q_α) ]               │
└──────────────────────────────────────────────────────────────────────────┘
                                  │
                                  ▼
OUTPUT
┌──────────────────────────────────────────────────────────────────────────┐
│ Final prediction:  p̂                                                     │
│ Uncertainty:      model band [q05,q95], conformal CI [L,U]               │
│ Provenance:       w*, b*, f_iso version, ε, λ, T                         │
└──────────────────────────────────────────────────────────────────────────┘

Legend:  σ(x)=1/(1+e^(−x)); logit(p)=ln(p/(1−p)); clip(p,ε,1−ε) bounds logits.
