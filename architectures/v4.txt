v4

────────────────────────────────────────────────────────────────────────
M1: BetaReg-MLP (Deep Net)
────────────────────────────────────────────────────────────────────────
INPUTS
  [ x_cell ] 
  [ x_capsid ] 
  [ x_promoter ]
        │
        ▼
    [ CONCAT ]
        │
    [ LayerNorm ]
        │
        ▼
SUBCOMPONENTS
  [ Dense 896→512 ] → [ SiLU ] → [ Dropout ]
        │
        ▼
  [ Dense 512→256 ] → [ SiLU ] → [ Dropout ]
        │
        ▼
  [ Dense 256→128 ] → [ SiLU ]
        │
        ▼
OUTPUT HEAD
  [ Linear 128→2 ] → [ Softplus ] → (α̂, β̂)
        │
        ▼
OUTPUT
  Beta(α̂,β̂) → Expected % expression p̂
               + Confidence Interval (uncertainty)



────────────────────────────────────────────────────────────────────────
M2: Cross-Attention Network
────────────────────────────────────────────────────────────────────────

x_cell (cell embedding) ────────────────┐
                                        │
                                        ▼
                             [ Linear Projection → Query (Q) ]

x_capsid (capsid embedding) ─┐
                             ├──► [ Linear Projection → Keys (K) ]
x_promoter (promoter vector) ─┘
                             └──► [ Linear Projection → Values (V) ]

Query (Q) ───────────────────────────────────────┐
Keys (K), Values (V) ────────────────────────────┘
                                        │
                                        ▼
                               [ Multi-Head Attention ]
                                        │
                                        ▼
                               [ Residual + LayerNorm ]
                                        │
                                        ▼
                               [ Feedforward Layer ]
                                        │
                                        ▼
                               [ Residual + LayerNorm ]
                                        │
                                        ▼
Concatenate (attn_out + x_cell + x_capsid + x_promoter)
                                        │
                                        ▼
                                   [ LayerNorm ]
                                        │
                                        ▼
                        [ Dense →128 ] → [ SiLU ] → [ Linear 128→2 ]
                                        │
                                        ▼
                              [ Softplus → (α̂, β̂) ]
                                        │
                                        ▼
OUTPUT: Beta(α̂,β̂)
        • Expected % expression (p̂ = α̂/(α̂+β̂))
        • Confidence Interval (uncertainty)


────────────────────────────────────────────────────────────────────────
M3: Gradient-Boosted Trees
────────────────────────────────────────────────────────────────────────

LEARNED EMBEDDINGS
 ┌───────────────────────────────┐
 │ x_cell (from scVI→MLP)        │
 │ x_capsid (Evo2∥ESM2→fuse)     │
 │ x_promoter (meta MLP ± CNN)   │
 └───────────────────────────────┘
            │
            ├──────────────┐
            │              │
            ▼              ▼
HANDCRAFTED FEATURE DERIVATION             METADATA DIRECT
 ┌───────────────────────────────┐         ┌───────────────────────────────┐
 │ cosine(cell,capsid)           │         │ tissue flags (one-hot)        │
 │  = dot/‖·‖‖·‖                 │         │ promoter strength (scaled)    │
 └───────────────────────────────┘         │ MOI → log1p                   │
            │                              │ timepoint → log1p / bins      │
            ▼                              └───────────────────────────────┘
                 ┌────────────────────────────────────────────────────────┐
                 │                 FEATURE MATRIX  Z                      │
                 │ concat[ x_cell, x_capsid, x_promoter,                  │
                 │         cosine_sim, vec_tissue, strength,              │
                 │         moi_log, t_feat ]                              │
                 └────────────────────────────────────────────────────────┘
                                             │
                                             ▼
                                 GRADIENT-BOOSTED TREES (XGBoost)
                                 ┌────────────────────────────────┐
                                 │ Tree #1  Tree #2  …  Tree #T   │
                                 └────────────────────────────────┘
                                             │
                                             ▼
                                  ENSEMBLE SUM → raw_score = logit(y)
                                             │
                                             ▼
                            OPTIONAL CALIBRATION (Platt / Isotonic)
                                             │
                                             ▼
                                            OUTPUT
                            ┌───────────────────────────────┐
                            │ p̂ = sigmoid(raw_score)        │
                            │ (calibrated probability)      │
                            └───────────────────────────────┘



────────────────────────────────────────────────────────────────────────
M4: Sparse Gaussian Process (RBF)
────────────────────────────────────────────────────────────────────────

INPUTS
 ┌───────────────────────────────┐
 │ x_cell, x_capsid, x_promoter  │
 └───────────────────────────────┘
              │
              ▼
 [ CONCAT ] → high-dim vector (896D)
              │
              ▼
 ┌───────────────────────────────┐
 │ PCA REDUCTION                 │
 │ • 896D → 32D (x₃₂)            │
 └───────────────────────────────┘
              │
              ▼
 ┌────────────────────────────────────────────┐
 │   SPARSE GAUSSIAN PROCESS (FITC approx)    │
 │                                            │
 │  [1] Inputs: x₃₂, inducing points Z        │
 │  [2] Kernels: RBF + White noise            │
 │  [3] Covariance: K_mm, K_nm, K_nn approx   │
 │  [4] Variational / FITC inference          │
 │  [5] Prediction: μ(x), σ²(x)               │
 │  [6] Link: sigmoid(μ) → p̂                  │
 └────────────────────────────────────────────┘
              │
              ▼
 OUTPUTS
   • p̂ = predicted probability
   • σ²(x) = uncertainty



────────────────────────────────────────────────────────────────────────
M5: Linear Probe (Ridge)
────────────────────────────────────────────────────────────────────────

INPUTS (learned embeddings)
┌──────────────────────────────────────────┐
│ x_cell  │ x_capsid  │ x_promoter         │
└──────────────────────────────────────────┘
                │
                ▼
[ CONCAT ] → x = [x_cell ⊕ x_capsid ⊕ x_promoter]
                │
                ▼
┌──────────────────────────────────────────┐
│ STANDARDIZE x                            │
│  • zero-mean, unit-variance per feature  │
└──────────────────────────────────────────┘
                │
                ▼
┌──────────────────────────────────────────┐
│ LINEAR MAP                               │
│  • s = w·x + b                           │
└──────────────────────────────────────────┘
                │
                ▼
┌──────────────────────────────────────────┐
│ SIGMOID                                  │
│  • p̂ = σ(s)                              │
└──────────────────────────────────────────┘
                │
                ├───────────────► INFERENCE OUTPUT
                │                 ┌──────────────────────────────┐
                │                 │ p̂ (baseline probability)     │
                │                 └──────────────────────────────┘
                │
                ▼
TRAINING (uses computational label y)
┌──────────────────────────────────────────────────────────────────────┐
│ LABEL FACTORY (computational y)                                      │
│  inputs: x_cell, x_capsid, x_promoter, strength, MOI, time, tissue   │
│  priors:                                                             │
│    • P_cc = σ(α·cos(x_cell, x_capsid))                               │
│    • P_pr = normalize(strength; tissue context)                      │
│    • A   = g(MOI, time)                                              │
│  fusion (monotone):                                                  │
│    • s* = w1·logit(P_cc) + w2·logit(P_pr) + w3·A + b*                │
│    • y  = σ(s*) ∈ [0,1]                                              │
└──────────────────────────────────────────────────────────────────────┘
                │
                ▼
┌──────────────────────────────────────────────────────────────────────┐
│ LOSS                                                                 │
│  compare p̂ vs y (from label factory)                                 │
│   • Logistic:  𝓛_cls = −[y log p̂ + (1−y) log(1−p̂)]                   │
│   • (or) Squared on score:  𝓛_reg = (y − s)²                         │
│  ridge penalty:  𝓛_ridge = λ‖w‖²                                     │
│  total:  𝓛 = 𝓛_cls/𝓛_reg + 𝓛_ridge                                   │
└──────────────────────────────────────────────────────────────────────┘
                │
                ▼
┌──────────────────────────────────────────────────────────────────────┐
│ SOLVER (minimizes 𝓛 over w,b)                                        │
│  • closed-form ridge:  w = (XᵀX + λI)⁻¹ Xᵀy  (small/medium X)        │
│  • coordinate descent / CG / SGD  (high-dim)                         │
│  updates w,b in [LINEAR MAP]                                         │
└──────────────────────────────────────────────────────────────────────┘


─────────────────────────────────────────────────────────────────────────────
STACKING BLENDER
─────────────────────────────────────────────────────────────────────────────

INPUTS (from M1-M5 base models)
 ┌─────────────────────────────────────────────────────────────────────────┐
 │ Level-0 predictions:  p₁, p₂, p₃, p₄, p₅                               │
 │ Optional uncertainties:                                                 │
 │   • M1/M2: Beta(α̂,β̂)  → samples or logit-mean/var                    │
 │   • M4:     Normal(μ,σ²) on logit → samples                            │
 │   • M3/M5:  point probs (optionally add tiny logit noise)              │
 └─────────────────────────────────────────────────────────────────────────┘
                                   │
                                   ▼
A) META-LEARNING (STACKING)
 ┌─────────────────────────────────────────────────────────────────────────┐
 │ OOF feature build:  zᵢ^(m) = logit(clip(pᵢ^(m), ε, 1−ε))               │
 │ Meta-model: constrained ridge on logits                                 │
 │   sᵢ = b + Σₘ wₘ zᵢ^(m),   w = softmax(u)  (⇒ wₘ ≥ 0, Σw=1)            │
 │   minimize  Σᵢ ( yᵢ − σ(sᵢ) )² + λ‖w‖²                                 │
 │ Store: weights w*, bias b*                                              │
 └─────────────────────────────────────────────────────────────────────────┘
                                   │
                                   ▼
B) UNCERTAINTY PROPAGATION (INFERENCE)
 ┌─────────────────────────────────────────────────────────────────────────┐
 │ Monte-Carlo blending (T samples):                                        │
 │   For t=1..T:                                                            │
 │     • draw p_t^(m) from each model’s predictive law                      │
 │     • z_t^(m)=logit(clip(p_t^(m)))                                       │
 │     • s_t = b* + Σₘ w*ₘ z_t^(m)                                          │
 │     • p_blend,t = σ(s_t)                                                 │
 │ Aggregate:  mean  \bar p  and var  Var[p_blend]                          │
 │ (Alt: delta method on logits if variances vₘ known: Var[s]≈Σ w*ₘ² vₘ)   │
 └─────────────────────────────────────────────────────────────────────────┘
                                   │
                                   ▼
C) CALIBRATION & INTERVALS
 ┌─────────────────────────────────────────────────────────────────────────┐
 │ Isotonic calibration on held-out:  p̂ = f_iso( \bar p )                  │
 │ Apply to samples:  p̂_t = f_iso(p_blend,t)                               │
 │ Quantile band (model variance):  [q05, q95] from {p̂_t}                  │
 │ Split-conformal coverage (by stratum, e.g., cell_type):                  │
 │   r_j = | y_j − f_iso(p_blend,j) |;   q_α = quantile_{1−α}( {r_j} )      │
 │   Final CI:  [ max(0, p̂ − q_α),  min(1, p̂ + q_α) ]                      │
 └─────────────────────────────────────────────────────────────────────────┘
                                   │
                                   ▼
OUTPUT
 ┌─────────────────────────────────────────────────────────────────────────┐
 │ Final prediction:  p̂                                                   │
 │ Uncertainty:     model band [q05, q95], conformal CI [L, U]             │
 │ Provenance:      blender weights w*, bias b*, calib f_iso version       │
 └─────────────────────────────────────────────────────────────────────────┘
